{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "saving_models.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2FAxX2B9HD3"
      },
      "source": [
        "# Saving and loading models\n",
        "\n",
        "In the previous lab you probably noticed it can take a long time to train these models. It is therefore very useful to be able to save models to disk so they can be reused."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBTHUuhT9NDP"
      },
      "source": [
        "Before being able to save the models, we need something to save it to. Colab notebooks can easily interact with your Google Drive. To enable this, your drive first has to be mounted:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HULa9z1v5gpz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "168e5b01-2dd2-4a66-aee9-91800b9a205e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NXVIG8e9pWP"
      },
      "source": [
        "We can now write and read from the Drive. First, let's create a folder to work in"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5wlvWgN-eXR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6a32727-4b14-4a89-abc1-d0f7dc18918e"
      },
      "source": [
        "import os\n",
        "\n",
        "folder_path = './gdrive/My Drive/thisfoldernamesurelydoesnotalreadyexistinyourdrive24/'\n",
        "\n",
        "if not os.path.isdir(folder_path):\n",
        "  os.mkdir(folder_path)\n",
        "  print('Folder created!')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75nOS_I7A9aI"
      },
      "source": [
        "We use this folder store the files used in this demo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hosJadj6VHU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "769cf442-6df5-4844-bc4d-ec1a0a65b8c1"
      },
      "source": [
        "with open(folder_path + 'some_file.txt', 'w') as f:  # Write something to the file (overwriting current file content)\n",
        "  f.write('foo')\n",
        "\n",
        "with open(folder_path + 'some_file.txt', 'a') as f:  # Append to existing content in the file\n",
        "  f.write('bar')\n",
        "\n",
        "with open(folder_path + 'some_file.txt', 'r') as f:  # Read from the file\n",
        "  print(f.read())\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "foobar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t53GYhK7BJ-L"
      },
      "source": [
        "Now suppose we have some model that we'd like to reuse and thus want save to disk. In this example we do not use a trained model, but the same methods work for trained models as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1kzncotGbIU"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VERvA0SF6VNF"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size, layer_sizes, output_size):\n",
        "    super(MLP, self).__init__()\n",
        "    shape = (input_size,) + tuple(layer_sizes) + (output_size,)\n",
        "    self.layers = nn.ModuleList([nn.Linear(shape[i], shape[i + 1]) for i in range(len(shape) - 1)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.layers[:-1]:\n",
        "      x = F.relu(layer(x))\n",
        "    return F.softmax(self.layers[-1](x), dim=1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_O0QnyU6VKR"
      },
      "source": [
        "model = MLP(784, (32, 32), 10)  # Initialize a new model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSQl2PjxlcbA"
      },
      "source": [
        "We can save the entire model object as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCqWf_bi7ixa"
      },
      "source": [
        "torch.save(model, folder_path + 'model.pth')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijymtmF8ljsQ"
      },
      "source": [
        "The model can then be loaded from disk by"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WZ_4sug7i7p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55aea378-ad50-46cb-adb2-798c1bb69df6"
      },
      "source": [
        "model = torch.load(folder_path + 'model.pth', weights_only=False)\n",
        "print(model)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  (layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=32, bias=True)\n",
            "    (1): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (2): Linear(in_features=32, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uWD-UPmlsko"
      },
      "source": [
        "A disadvantage of this method is that this might break when changes are made to the model class or when the directories containing the class definitions are restructured. This is because only the location of the model class definition is stored, instead of the actual code. An alternative way of saving/loading models is to only store the weights. These weights are contained in the model's state dict. Saving and loading models then works as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NoBKfgs7i-w"
      },
      "source": [
        "torch.save(model.state_dict(), folder_path + 'model_state.pth')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FccdPbxM6VEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "291ed131-4755-4799-a8e9-13dd9039e040"
      },
      "source": [
        "model_state = torch.load(folder_path + 'model_state.pth')  # Load the model's weights\n",
        "model = MLP(784, (32, 32), 10)  # It is required to have a model object to set its weights, so we initialize a new one\n",
        "model.load_state_dict(model_state)  # Set the model weights to the saved weights. Note that the model architecture must be equal to the architecture of the model from which the state_dict was obtained!"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAHxfwIyncgr"
      },
      "source": [
        "Similarly, it is possible to save the state of an optimizer. Suppose you want\n",
        "\n",
        "---\n",
        "\n",
        "to stop training a model, but have the possibility of continuing later. This requires you to save the optimizer state as well (think for example of the velocity term when using momentum in SGD)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GbvlUg19F39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdffa2c9-a93d-4a80-d954-6b7f39f7e79c"
      },
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "\n",
        "torch.save(optimizer.state_dict(), folder_path + 'optimizer_state.pth')\n",
        "\n",
        "optimizer_state = torch.load(folder_path + 'optimizer_state.pth')\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=42)\n",
        "\n",
        "optimizer.load_state_dict(optimizer_state)\n",
        "\n",
        "print(optimizer)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    differentiable: False\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.01\n",
            "    maximize: False\n",
            "    momentum: 0.5\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sibsvA-K9F2t"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}